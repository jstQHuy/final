import tiktoken
from llama_index.core.callbacks import CallbackManager, TokenCountingHandler
from utils.doc_handler_ec import DocHandler
from llama_index.llms.openai import OpenAI
from llama_index.llms.langchain import LangChainLLM
from llama_index.core import Settings
import subprocess
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import pandas as pd
from sklearn.metrics import confusion_matrix, cohen_kappa_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, spearmanr
from itertools import combinations
import numpy as np
import json
import re
import logging
import time
from tqdm import tqdm
from utils.searchers.search_once import compose
import yaml
from utils.model_manager import get_model
logger = logging.getLogger(__name__)

config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'configs', 'config.yaml')
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)
planning_config = config['runtime']['planning']
model_name_for_token = config['models']['openai']['model']

def cvemap_search(cve, info_dir):
    """
    Replacement for legacy cvemap enrichment: use ProjectDiscovery 'vulnx search'.
    Writes a backward-compatible file: <info_dir>/cvemap.json as a list with one dict.
    Downstream code expects keys such as: cve_id, cve_description, weaknesses, cvss_score, epss.
    """
    import shutil

    os.makedirs(info_dir, exist_ok=True)
    cvemap_json_path = f"{info_dir}/cvemap.json"

    vulnx_bin = shutil.which("vulnx") or os.path.join(os.path.expanduser("~"), "go", "bin", "vulnx")
    if not (vulnx_bin and os.path.exists(vulnx_bin)):
        logging.error("vulnx binary not found in PATH or ~/go/bin. Skipping vulnx enrichment.")
        return None

    cve = str(cve).strip().upper()
    if not re.fullmatch(r"CVE-\d{4}-\d{4,7}", cve):
        logging.error(f"Invalid CVE id: {cve}")
        return None

    cmd = [vulnx_bin, "search", cve, "--limit", "1", "--json"]

    try:
        r = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=60)
        raw = (r.stdout or "").strip()
        if not raw:
            logging.error(f"Empty vulnx result: {cve}")
            return None

        # Best-effort parse and normalize
        rec = {
            "cve_id": cve,
            "cve_description": "",
            "weaknesses": [],
            "cvss_score": 0,
            "epss": {"epss_score": 0, "epss_percentile": 0},
        }

        try:
            obj = json.loads(raw)
        except Exception:
            obj = None

        def walk(x):
            if isinstance(x, dict):
                # prefer description-like fields
                for k in ("cve_description", "description", "summary", "title"):
                    v = x.get(k)
                    if isinstance(v, str) and len(v.strip()) > len(rec["cve_description"]):
                        rec["cve_description"] = v.strip()
                # weaknesses / cwe
                for k in ("weaknesses", "cwe", "cwes"):
                    v = x.get(k)
                    if isinstance(v, list) and v and not rec["weaknesses"]:
                        rec["weaknesses"] = v
                    elif isinstance(v, str) and v and not rec["weaknesses"]:
                        rec["weaknesses"] = [v]
                # cvss
                for k in ("cvss_score", "cvss", "cvssScore"):
                    v = x.get(k)
                    if isinstance(v, (int, float)):
                        rec["cvss_score"] = float(v)
                    elif isinstance(v, str):
                        try:
                            rec["cvss_score"] = float(v)
                        except Exception:
                            pass
                # epss
                epss = x.get("epss")
                if isinstance(epss, dict):
                    s = epss.get("epss_score")
                    p = epss.get("epss_percentile")
                    try:
                        if s is not None:
                            rec["epss"]["epss_score"] = float(s)
                    except Exception:
                        pass
                    try:
                        if p is not None:
                            rec["epss"]["epss_percentile"] = float(p)
                    except Exception:
                        pass
                for v in x.values():
                    walk(v)
            elif isinstance(x, list):
                for it in x:
                    walk(it)

        if obj is not None:
            walk(obj)

        with open(cvemap_json_path, "w", encoding="utf-8") as f:
            json.dump([rec], f, indent=2, ensure_ascii=False)

        return [rec]

    except subprocess.TimeoutExpired:
        logging.error(f"vulnx search timed out: {cve}")
    except subprocess.CalledProcessError as e:
        logging.error(f"vulnx search failed for {cve}: {(e.stderr or '').strip()}")
    except Exception as e:
        logging.error(f"Failed to write vulnx json for {cve}: {e}")

    return None

def searchsploit_search(cve):
    searchsploit_json_path = f"resources/{cve}/exploitdb.json"
    searchsploit_command = f"searchsploit {cve} -j > {searchsploit_json_path}"
    try:
        subprocess.run(
            searchsploit_command,
            shell=True,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=50,
        )
    except FileNotFoundError:
        logging.error("searchsploit binary not found. Install exploitdb or disable ExploitDB usage.")
        return {}
    except subprocess.TimeoutExpired:
        logging.error(f"searchsploit timed out: {cve}")
        return {}
    except subprocess.CalledProcessError as e:
        logging.error(f"searchsploit failed for {cve}: {e.stderr.strip()}")
        return {}

    try:
        with open(searchsploit_json_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"Failed to read searchsploit json for {cve}: {e}")
        return {}
    

def categorize_cvss(cvss_score):
    if not (0.0 <= cvss_score <= 10.0):
        return "value not in range"

    if cvss_score < 4.0:
        return "hard"
    elif 4.0 <= cvss_score < 7.0:
        return "medium"
    else:
        return "easy"
    
def categorize_epss(epss_score):
    if not (0.0 <= epss_score <= 1.0):
        return "value not in range"

    if epss_score < 0.4:
        return "hard"
    elif 0.4 <= epss_score < 0.94:
        return "medium"
    else:
        return "easy"
    
def count_cwe(cve_lst):
    
    cwe_dict = {}
    for cve in cve_lst:
        info_dir = f"resources/{cve}/info"
        cvemap_json_path = f"{info_dir}/cvemap.json"
        with open(cvemap_json_path) as f:
            cvemap_json = json.load(f)[0]
            cwe_lst = cvemap_json.get('weaknesses', [])
            for cwe in cwe_lst:
                cwe_id = cwe['cwe_id']
                if cwe_id in cwe_dict:
                    cwe_dict[cwe_id] += 1
                else:
                    cwe_dict[cwe_id] = 1
    return cwe_dict

def calculate_score(features, trending_score):
    weights = config['cve_scoring']['weights']

    scores = {}
    max_score_github = 0
    max_score_repo_github = None
    max_score_expdb = 0
    max_score_repo_expdb = None
    max_score = 0
    max_score_repo = None
    final_score = 0
    has_code = False

    code_sources = ["GitHub", "ExploitDB"]

    code_results = None
    doc_results = None

    if features['code'].get('GitHub') or features['code'].get('ExploitDB'):
        for source in code_sources:
            if not features['code'].get(source):
                continue  # if not have data from source, skip

            has_code = True
            for repo, score in features['code'][source]['score'].items():
                if repo == "Code_File":
                    score = score / 2

                score *= weights["lang_class"].get(features['code'][source]["lang_class"].get(repo, ""), 1)

                if source == "GitHub":
                    score *= weights["source_weights"]["gthb"]
                elif source == "ExploitDB":
                    score *= weights["source_weights"]["expdb"]

                # Store the score
                scores[repo] = score

                # Check for max score
                if repo.isdigit():  # ExploitDB
                    if score >= max_score_expdb:
                        max_score_expdb = score
                        max_score_repo_expdb = repo
                else:  # GitHub
                    if score >= max_score_github:
                        max_score_github = score
                        max_score_repo_github = repo

        if max_score_expdb > max_score_github:
            max_score_repo = max_score_repo_expdb
            max_score = max_score_expdb
        else:
            max_score_repo = max_score_repo_github
            max_score = max_score_github

        if has_code:
            sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)

            if trending_score == 999:
                trending_score_github = 0
                trending_score_expdb = weights["expdb_default_score"]
            else:
                trending_score_github = trending_score
                if features['code'].get('ExploitDB'):
                    trending_score_expdb = weights["expdb_default_score"]
                else: trending_score_expdb = 0

            # Add weighted trending score to the max score
            trend_score_weighted_expdb = trending_score_expdb * weights["trending_score"]
            trend_score_weighted_github = trending_score_github * weights["trending_score"]
    
            # Handle cases where one source has no scores
            final_expdb = (max_score_expdb + trend_score_weighted_expdb) if max_score_expdb > 0 else 0
            final_github = (max_score_github + trend_score_weighted_github) if max_score_github > 0 else 0

            final_score = max(final_expdb, final_github)

            code_results = (sorted_scores, max_score_repo, max_score, final_score, has_code)
    
    if features['doc']:
        doc_score = features['doc']['score']
        
        doc_score *= weights["source_weights"]["gg"]
        
        scores["doc"] = doc_score

        # Check if 'doc' has the max score
        if doc_score > max_score:
            max_score = doc_score
            max_score_repo = "doc"
            final_score = doc_score

        doc_results = (scores, max_score_repo, max_score, max_score, has_code)
    
    if code_results and doc_results:
        code_sorted_scores, code_max_repo, code_max_score, code_final_score, code_has_code = code_results
        doc_scores, doc_max_repo, doc_max_score, doc_final_score, doc_has_code = doc_results
        
        # merge scores
        all_scores = dict(code_sorted_scores)
        all_scores.update(doc_scores)
        sorted_all_scores = sorted(all_scores.items(), key=lambda item: item[1], reverse=True)
        
        # final max score and response repo
        if doc_max_score > code_max_score:
            final_max_score = doc_max_score
            final_max_repo = doc_max_repo
            final_score = doc_max_score
        else:
            final_max_score = code_max_score
            final_max_repo = code_max_repo
            final_score = code_final_score
            
        return sorted_all_scores, final_max_repo, final_max_score, final_score, has_code or True
        
    elif code_results:
        return code_results
    elif doc_results:
        return doc_results
    else:
        return None, None, max_score, final_score, has_code
    
def calculate_match_stats(values1, values2):
    total = len(values1)
    exact_matches = sum(v1 == v2 for v1, v2 in zip(values1, values2))
    near_matches = sum(
        abs(['easy', 'medium', 'hard'].index(v1) - ['easy', 'medium', 'hard'].index(v2)) == 1
        for v1, v2 in zip(values1, values2)
    )
    mismatches = total - exact_matches - near_matches
    return {
        "Exact Match (%)": (exact_matches / total) * 100,
        "Near Match (%)": (near_matches / total) * 100,
        "Mismatch (%)": (mismatches / total) * 100,
    }

def create_df(cvss_data, epss_data, pentestasst_data):
    all_keys = sorted(set(cvss_data.keys()).union(epss_data.keys(), pentestasst_data.keys()))
    data = {
        'CVSS': [cvss_data.get(key, np.nan) for key in all_keys],
        'EPSS': [epss_data.get(key, np.nan) for key in all_keys],
        'EEAS': [pentestasst_data.get(key, np.nan) for key in all_keys],
    }
    df = pd.DataFrame(data, index=all_keys)
    return df

def normalize_data(df):
    if len(df) == 1:
        # for the case of a single value, directly return 0.5 or keep the original value
        df[['EEAS']] = 0.5  # or df[['EEAS']]
    else:
        scaler = StandardScaler()
        minmax_scaler = MinMaxScaler()
        df[['EEAS']] = minmax_scaler.fit_transform(scaler.fit_transform(df[['EEAS']]))
    normalized_df = pd.DataFrame(df, index=df.index, columns=df.columns)
    return normalized_df
    
def bin_agreement_analysis(df):
    pairwise_stats = {}
    for (name1, col1), (name2, col2) in combinations(df.items(), 2):
        # Calculate confusion matrix
        cm = confusion_matrix(col1, col2, labels=['easy', 'medium', 'hard'])
        kappa = cohen_kappa_score(col1, col2, labels=['easy', 'medium', 'hard'])
        stats = calculate_match_stats(col1, col2)

        pairwise_stats[f"{name1} vs {name2}"] = {
            "Confusion Matrix": cm.tolist(),
            "CohenS Kappa": kappa,
            **stats,
        }

    # Display results
    for pair, results in pairwise_stats.items():
        print(f"\nPairwise comparison: {pair}")
        print(f"Confusion Matrix:\n{np.array(results['Confusion Matrix'])}")
        print(f"Cohen's Kappa: {results['CohenS Kappa']:.4f}")
        print(f"Exact Match (%): {results['Exact Match (%)']:.2f}")
        print(f"Near Match (%): {results['Near Match (%)']:.2f}")
        print(f"Mismatch (%): {results['Mismatch (%)']:.2f}")

    return df

def num_agreement_analysis(df):
    stats = {}
    for col1, col2 in combinations(df.columns, 2):
        mean_values = (df[col1] + df[col2]) / 2
        diff_values = df[col1] - df[col2]

        mean_diff = np.mean(diff_values)
        std_diff = np.std(diff_values)

        print(f"diff mean: {np.mean(diff_values)}")
        print(f"diff std: {np.std(diff_values)}")

        threshold = 1.96 * std_diff
        diff_condition = abs(df[col1] - df[col2]) > threshold

        rows_large_diff = df[diff_condition]
        print(f"Rows where the difference between {col1} and {col2} is larger than {threshold}:")
        print(rows_large_diff)
        print(f"Total rows: {len(df)}, Rows with large difference: {len(rows_large_diff)}, percentage: {len(rows_large_diff)/len(df) * 100}%")
        
        values1, values2 = df[col1], df[col2]
        pearson_corr, _ = pearsonr(values1, values2)
        spearman_corr, _ = spearmanr(values1, values2)
        mae = np.mean(np.abs(values1 - values2))
        rmse = np.sqrt(np.mean((values1 - values2) ** 2))
        stats[f"{col1} vs {col2}"] = {
            "Pearson Correlation": pearson_corr,
            "Spearman Correlation": spearman_corr,
            "Mean Absolute Error (MAE)": mae,
            "Root Mean Squared Error (RMSE)": rmse,
        }

    for pair, metrics in stats.items():
        print(f"\nAgreement Analysis for {pair}:")
        for metric, value in metrics.items():
            print(f"{metric}: {value:.4f}")

    return df

def visualize_num_results(df, output_dir="plots/"):

    plt.rc('font', size=14) 

    # Pairwise scatter plots
    pairplot = sns.pairplot(df, kind="reg", diag_kind="kde")
    pairplot.savefig(os.path.join(output_dir, "pairwise_scatter_plots.pdf"))
    plt.close()

    # Heatmap for correlation
    correlation_matrix = df.corr()
    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Heatmap of Correlations")
    plt.savefig(os.path.join(output_dir, "correlation_heatmap.pdf"))
    plt.close()

    # Bland-Altman plots
    for col1, col2 in combinations(df.columns, 2):
        mean_values = (df[col1] + df[col2]) / 2
        diff_values = df[col1] - df[col2]

        plt.figure(figsize=(8, 6))
        plt.scatter(mean_values, diff_values, alpha=0.7)
        plt.axhline(np.mean(diff_values), color='red', linestyle='--', label='Mean Difference')
        plt.axhline(np.mean(diff_values) + 1.96 * np.std(diff_values), color='blue', linestyle='--', label='Â±1.96SD')
        plt.axhline(np.mean(diff_values) - 1.96 * np.std(diff_values), color='blue', linestyle='--', label=None)
        plt.title(f"Bland-Altman Plot: {col1} vs {col2}", weight='bold')
        plt.xlabel('Mean of Two Measurements', weight='bold')
        plt.ylabel('Difference', weight='bold')
        plt.legend()
        filename = f"bland_altman_{col1}_vs_{col2}.pdf"
        plt.savefig(os.path.join(output_dir, filename))
        plt.close()

def visualize_bin_results(cvss_data, epss_data, pentestasst_data, output_dir):
    # Confusion Matrix Heatmap
    for (name1, data1), (name2, data2) in combinations([("CVSS", cvss_data), ("EPSS", epss_data), ("EEAS", pentestasst_data)], 2):
        labels = ['easy', 'medium', 'hard']
        confusion = confusion_matrix(list(data1.values()), list(data2.values()), labels=labels)
        
        plt.figure(figsize=(6, 5))
        sns.heatmap(confusion, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
        plt.title(f"Confusion Matrix: {name1} vs {name2}")
        plt.xlabel(f"{name2} Predictions")
        plt.ylabel(f"{name1} Predictions")
        plt.savefig(f"{output_dir}/confusion_matrix_{name1}_vs_{name2}.pdf")
        plt.close()

    # Bar Chart of Class Distributions
    data = {
        "Category": ['easy', 'medium', 'hard'],
        "CVSS": [list(cvss_data.values()).count(c) for c in ['easy', 'medium', 'hard']],
        "EPSS": [list(epss_data.values()).count(c) for c in ['easy', 'medium', 'hard']],
        "EEAS": [list(pentestasst_data.values()).count(c) for c in ['easy', 'medium', 'hard']],
    }
    df = pd.DataFrame(data).set_index("Category")
    
    df.plot(kind="bar", figsize=(8, 6))
    plt.title("Class Distributions Across Dictionaries")
    plt.ylabel("Frequency")
    plt.xlabel("Category")
    plt.xticks(rotation=0)
    plt.savefig(f"{output_dir}/class_distributions.pdf")
    plt.close()

    # Pairwise Agreement Bar Chart
    pairwise_agreements = []
    for (name1, data1), (name2, data2) in combinations([("CVSS", cvss_data), ("EPSS", epss_data), ("EEAS", pentestasst_data)], 2):
        total = len(data1)
        exact_matches = sum(1 for k in data1 if data1[k] == data2[k])
        near_matches = sum(1 for k in data1 if abs(['easy', 'medium', 'hard'].index(data1[k]) -
                                                   ['easy', 'medium', 'hard'].index(data2[k])) == 1)
        mismatches = total - exact_matches - near_matches

        pairwise_agreements.append({
            "Pair": f"{name1} vs {name2}",
            "Exact Matches (%)": exact_matches / total * 100,
            "Near Matches (%)": near_matches / total * 100,
            "Mismatches (%)": mismatches / total * 100
        })

    pairwise_df = pd.DataFrame(pairwise_agreements).set_index("Pair")
    pairwise_df.plot(kind="bar", stacked=True, figsize=(10, 6))
    plt.title("Pairwise Agreement Percentages")
    plt.ylabel("Percentage")
    plt.xlabel("Pairwise Comparison")
    plt.xticks(rotation=0)
    plt.legend(loc="upper right")
    plt.savefig(f"{output_dir}/pairwise_agreements.pdf")
    plt.close()
    
def cve_classifier(cve, output_dir="resources/", mode = "specific"):
    trending_score_path = f"{output_dir}/{cve}/Trend_Score.json"
    feature_path = f"{output_dir}/{cve}/direct_scores.json"
    cvemap_path = f"{output_dir}/{cve}/info/cvemap.json"
    result = None
    with open(feature_path, 'r') as f:
        result = json.load(f)
    trending_score = 0
    if result['code'].get('GitHub'):
        try:
            with open(trending_score_path, 'r') as f:
                _trending_score = json.load(f)
                # print(trending_score)
                trending_score = min(_trending_score.get("trend_score", 0), 50)
        except:
            logging.warning(f"Trending score not found for {cve}")
    else:
        trending_score = 999

    if mode == "specific":
        cvemap_json = None
        with open(cvemap_path, 'r') as f:
            cvemap_json = json.load(f)[0]
        cvss_score = cvemap_json.get('cvss_score', 0)
        epss_score = cvemap_json['epss']['epss_score']
        epss_percentile = cvemap_json['epss']['epss_percentile']
    elif mode == "general":
        cvss_score = 0
        epss_score = 0
        epss_percentile = 0
    cvss_category = categorize_cvss(cvss_score)
    epss_category = categorize_epss(epss_percentile)

    scores, max_score_repo, max_score, final_score, has_code = calculate_score(result, trending_score)

    exploitability = "hard"
    if final_score is not None and final_score > 0:
        if final_score > 50:    
            exploitability = "easy"
        elif final_score > 35:
            exploitability = "medium"
        else:
            exploitability = "hard"
    

    with open(f"{output_dir}/{cve}/classification_ec.json", 'w') as f:
        classification = {
            "cvss_category": cvss_category,
            "epss_category": epss_category,
            "exploitability": exploitability,
            "final_score": final_score,
            "max_score_repo": max_score_repo,
            "max_score": max_score,
            "scores": scores,
            "has_code": has_code
        }
        json.dump(classification, f, indent=4)

    return cvss_score, cvss_category, epss_percentile, epss_category, final_score, exploitability, has_code

def cve_analysis(cve, output_dir="resources/"):
    info_dir = f"{output_dir}/{cve}/info"
    searching_start_time = time.time()
    compose(output_dir, cve)
    if not os.path.exists(info_dir):
        os.mkdir(info_dir)
    cvemap_json = cvemap_search(cve, info_dir)
    if cvemap_json is None:
        # Keep interface stable: callers expect (searching_time, analysis_time)
        return 0.0, 0.0

    logging.info(f"Analyzing {cve}")
    doc_handler = DocHandler()

    analysis_start_time = time.time()
    result = doc_handler.vul_analysis(cve, output_dir)
    analysis_end_time = time.time()
    searching_time = analysis_start_time - searching_start_time
    analysis_time = analysis_end_time - analysis_start_time
    logging.info(f"Analysis time: {analysis_time} seconds")
    
    with open(f"{output_dir}/{cve}/direct_scores.json", 'w') as f:
        
        json.dump(result, f, indent=4)

    return searching_time, analysis_time

def general_analysis(keyword, output_dir="resources/"):
    searching_start_time = time.time()
    compose(output_dir, keyword, loose_mode = True)

    cve = keyword

    logging.info(f"Analyzing {keyword}")
    doc_handler = DocHandler()

    analysis_start_time = time.time()
    result = doc_handler.vul_analysis(cve, output_dir)
    analysis_end_time = time.time()
    searching_time = analysis_start_time - searching_start_time
    analysis_time = analysis_end_time - analysis_start_time
    logging.info(f"Analysis time: {analysis_time} seconds")
    
    with open(f"{output_dir}/direct_scores.json", 'w') as f:
        
        json.dump(result, f, indent=4)

    return searching_time, analysis_time

def cve_analysis_from_epss_csv(csv_path):
    
    df = pd.read_csv(csv_path)
    # print(df.cve)
    with tqdm(total=len(df.index), desc=f'Analyzing CVEs') as pbar:
        for index, row in df.iterrows():
            cve = row['cve']
            if int(cve.split('-')[1]) < 2017 or int(cve.split('-')[1]) > 2022:
                logger.info(f"Skipping {cve}")
                pbar.update()
                continue
            try:
                cve_analysis(cve)
                time.sleep(1)
                pbar.update()
                logging.info(f"Finished {index}: {cve}")
            except Exception as e:
                logging.error(f"Error in {cve}: {e}")
                pbar.update()
                continue

def product_to_cve(product, output_dir):
    cve_lst = []
    product_dir_name = product.lower().replace(" ", "_").replace("/", "_").replace(":", "_").replace("\\", "_").replace("(", "_").replace(")", "_").replace('"', "").replace("'", "").replace("\n", "").replace("&", "")
    if os.path.exists(f"{output_dir}/{product_dir_name}"):
        with open(f"{output_dir}/{product_dir_name}/cve_lst.json") as f:
            cve_lst = json.load(f)
        return cve_lst

    os.makedirs(f"{output_dir}/{product_dir_name}", exist_ok=True)
    cleaned_product = product_keyword_gen_openai(product, 3)
    # print(cleaned_product)
    cleaned_product.append(product_dir_name)
    
    for p in cleaned_product:

    
        # Safe filename component
    
        p_dir_name = re.sub(r"[^a-z0-9_.-]+", "_", str(p).lower()).strip("_")
    
        cvemap_json_path = f"{output_dir}/{product_dir_name}/cvemap_{p_dir_name}.json"

    
        import shutil
    
        vulnx_bin = shutil.which("vulnx") or os.path.join(os.path.expanduser("~"), "go", "bin", "vulnx")
    
        if not (vulnx_bin and os.path.exists(vulnx_bin)):
    
            raise FileNotFoundError("vulnx not found in PATH or ~/go/bin")

    
        cmd = [vulnx_bin, "search", str(p), "--limit", "200", "--json"]
    
        r = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=90)

    
        cve_ids = sorted(set(x.upper() for x in re.findall(r"\bCVE-\d{4}-\d{4,7}\b", r.stdout or "")))
    
        json_data = [{"cve_id": c} for c in cve_ids]

    
        # Write compat file; downstream code reads it immediately
    
        with open(cvemap_json_path, "w", encoding="utf-8") as wf:
    
            json.dump(json_data, wf, indent=2, ensure_ascii=False)
        # Append to CVE list (compat schema: [{'cve_id': ...}, ...])
        for _c in json_data:
            if isinstance(_c, dict) and _c.get('cve_id'):
                cve_lst.append(_c['cve_id'])
        
        with open(f"{output_dir}/{product_dir_name}/cve_lst.json", 'w') as f:
            json.dump(cve_lst, f, indent=4)
    
    return cve_lst

def product_keyword_gen_openai(product, num):
    prompt = (
        "You're an excellent system administrator. You will be given a product name, which may not be in a standard format. "
        "Your task is to generate alternative product names to broaden the search scope.\n"
        f"The given product name is {product}, please generate at most {num} alternative product names in the following format.\n"
        "[Alternative product name 1, Alternative product name 2, Alternative product name 3, ...]"
    )
    try:
        model_name = planning_config['model']
        llm = get_model(model_name)
        from langchain_core.messages import HumanMessage
        response = llm.invoke([HumanMessage(content=prompt)])
        product_lst_str = response.content.strip() if hasattr(response, 'content') else str(response)
    except Exception as e:
        import openai
        client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": prompt},
            ],
        )
        product_lst_str = str(response.choices[0].message.content)
    product_lst = product_lst_str.strip("[]").replace("'", "").split(", ")
    # print(product_lst)
    product_lst = [product.lower().replace(" ", "_").replace("/", "_").replace(":", "_").replace("\\", "_").replace("(", "_").replace(")", "_").replace('"', "").replace("'", "").replace("\n", "").replace("&", "") for product in product_lst]

    return product_lst

def product_keyword_gen_huggingface(product, num):

    messages = [
        {"role": "system", "content": "You're an excellent system administrator. You will be given a product name, which may not be in a standard format. Your task is to generate alternative product names to broaden the search scope."},
        {"role": "user", "content": f"The given product name is {product}, please generate {num} alternative product names in the following format: [Alternative product name 1, Alternative product name 2, Alternative product name 3, ...]"},
    ]

    # Run with Local LLM
    product_lst = chat_completion_huggingface(
        model_name="meta-llama/Meta-Llama-3.1-8B-Instruct",
        messages=messages,
        max_tokens=200,
        temperature=0.0
    )

    return product_lst

def chat_completion_huggingface(model_name="meta-llama/Meta-Llama-3.1-8B-Instruct", messages=None, max_tokens=100, temperature=0.0, top_p=0.0):
    """
    Generates a chat completion using a specified Hugging Face model.

    Args:
        model_name (str): The Hugging Face model name (default is Mistral-7B-Instruct).
        messages (list): A list of chat messages in OpenAI format [{"role": "user", "content": "Hello"}].
        max_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature (higher = more creative, lower = more deterministic).
        top_p (float): Nucleus sampling parameter (controls randomness).

    Returns:
        str: The model's response.
    """

    # Load the model and tokenizer dynamically
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")  # Uses GPU if available

    # Convert messages into a formatted prompt
    prompt = ""
    for message in messages:
        if message["role"] == "system":
            prompt += f"<s>[SYSTEM]: {message['content']}\n"
        elif message["role"] == "user":
            prompt += f"[USER]: {message['content']}\n"
        elif message["role"] == "assistant":
            prompt += f"[ASSISTANT]: {message['content']}\n"
    prompt += "[ASSISTANT]:"

    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Generate response
    output = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        do_sample=False
    )

    # Decode and return response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response.split("[ASSISTANT]:")[-1].strip()

def analyze_cve_lst(cve_lst, output_dir, app_name):
    cvss_results = {}
    cvss_scores = {}
    epss_results = {}
    epss_scores = {}
    pentestasst_results = {}
    pentestasst_scores = {}
    analysis_time_dict = {}
    has_code_lst = []
    total_searching_time = 0
    total_analysis_time = 0

    for cve in cve_lst:
        logging.info(f"Analyzing {cve}")
        if not os.path.exists(f"{output_dir}/{cve}/direct_scores.json"):
            searching_time, analysis_time = cve_analysis(cve, output_dir)
            if analysis_time == 0:
                continue
            total_searching_time += searching_time
            total_analysis_time += analysis_time
            analysis_time_dict[cve] = analysis_time
        cvss_score, cvss_category, epss_score, epss_category, final_score, exploitability, has_code = cve_classifier(cve, output_dir, mode = "specific")
        cvss_results[cve] = cvss_category
        cvss_scores[cve] = cvss_score
        epss_results[cve] = epss_category
        epss_scores[cve] = epss_score
        pentestasst_results[cve] = exploitability
        pentestasst_scores[cve] = final_score
        if has_code:
            has_code_lst.append(cve)

    with open(f"{output_dir}/analysis_time.json", 'w') as f:
        json.dump(analysis_time_dict, f, indent=4)

    # bin_df = create_df(cvss_results, epss_results, pentestasst_results)
    num_df = create_df(cvss_scores, epss_scores, pentestasst_scores)
    normalized_num_df = normalize_data(num_df)
    normalized_has_code_num_df = normalized_num_df.loc[has_code_lst]
    num_df.to_csv(f'{output_dir}/score_results.csv')
    normalized_num_df.to_csv(f'{output_dir}/normalized_score_results.csv')
    normalized_has_code_num_df.to_csv(f'{output_dir}/normalized_has_code_score_results.csv')


    if app_name:
        logging.info(f"Searching for general exp info for {app_name}")
        print("Start searching general exp info...")
        app_exp_path = app_name + "_exp"
        if not os.path.exists(f"{output_dir}/{app_exp_path}/direct_scores.json"):
            keyword = app_name + " exploit"
            searching_time, analysis_time = general_analysis(keyword, os.path.join(output_dir, app_exp_path))
            total_searching_time += searching_time
            total_analysis_time += analysis_time
        cve_classifier(app_exp_path, output_dir, mode = "general")

    avg_searching_time = total_searching_time / ((len(cve_lst) +1) if app_name else (len(cve_lst)))
    avg_analysis_time = total_analysis_time / ((len(cve_lst) +1) if app_name else (len(cve_lst)))
    logging.info(f"Total searching time: {total_searching_time} seconds, average time: {avg_searching_time} seconds")
    logging.info(f"Total analysis time: {total_analysis_time} seconds, average time: {avg_analysis_time} seconds")

    return total_searching_time, total_analysis_time


def get_exp_info(cve_lst = [], output_dir = "", app_name = ""):
    logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        filename='cve_info.log',
                        level=logging.INFO)
    
    track_tokens = False
    token_counter = None
    model_name = planning_config['model']
    if track_tokens:
        try:
            token_counter = TokenCountingHandler(tokenizer=tiktoken.encoding_for_model(model_name_for_token).encode)
            Settings.callback_manager = CallbackManager([token_counter])
        except Exception as e:
            logger.warning(f"Tokenizer for model {model_name_for_token} not found, token counting disabled: {e}")
            Settings.callback_manager = CallbackManager([])
    
    ### Step 1: Select the LLM

    ## Initialize LLM from config via model manager (provider-agnostic)
    try:
        model_name = planning_config['model']
    except Exception:
        model_name = "openai"

    llm = get_model(model_name)
    if llm is None:
        # Keep previous behavior when no API key/config: abort gracefully
        print("LLM_API_KEY not set")
        return

    try:
        Settings.llm = LangChainLLM(llm=llm)
    except Exception:
        # Fallback to OpenAI config if available to preserve behavior
        cve_config = config['cve']
        print(f"Model: {cve_config['model']}")
        Settings.llm = OpenAI(temperature=cve_config['temperature'], model=cve_config['model'])

    ### Step 2: Input the CVEs to analyze

    if not cve_lst:
        print("CVE to be searched not set! ")
        return
    

    ### Step 3: Analyze the CVEs
    if not output_dir:
        print("Directory to store not set! will use default settings.")
        output_dir = "resources_cve_lst_try"

    os.makedirs(output_dir, exist_ok=True)

    ## Scenario 2: Given a list of CVEs, analyze the CVEs

    total_searching_time, total_analysis_time = analyze_cve_lst(cve_lst, output_dir, app_name)


    if track_tokens:
        print(
            "Embedding Tokens: ",
            token_counter.total_embedding_token_count,
            "\n",
            "LLM Prompt Tokens: ",
            token_counter.prompt_llm_token_count,
            "\n",
            "LLM Completion Tokens: ",
            token_counter.completion_llm_token_count,
            "\n",
            "Total LLM Token Count: ",
            token_counter.total_llm_token_count,
            "\n",
            "Average LLM Token Count: ",
            token_counter.total_llm_token_count / len(cve_lst),
            "\n",
        )

        logging.info(
            "Embedding Tokens: "
            + str(token_counter.total_embedding_token_count)
            + "\n"
            + "LLM Prompt Tokens: "
            + str(token_counter.prompt_llm_token_count)
            + "\n"
            + "LLM Completion Tokens: "
            + str(token_counter.completion_llm_token_count)
            + "\n"
            + "Total LLM Token Count: "
            + str(token_counter.total_llm_token_count)
            + "\n"
            + "Average LLM Token Count: "
            + str(token_counter.total_llm_token_count / len(cve_lst))
            + "\n"
        )

        token_counter.reset_counts()

    return total_searching_time, total_analysis_time


def main():
    logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        filename='cve_info.log',
                        level=logging.INFO)
    
    track_tokens = False
    token_counter = None
    if track_tokens:
        token_counter = TokenCountingHandler(tokenizer=tiktoken.encoding_for_model("gpt-4o-mini").encode)
        Settings.callback_manager = CallbackManager([token_counter])
    
    ### Step 1: Select the LLM

    ## OpenAI API 
    api_key = os.environ.get("OPENAI_API_KEY")
    if api_key is None or api_key == "":
        print("OPENAI_API_KEY not set")
        return
    elif "f4Ph3uIoqGLC9" in api_key:
        print("Model: o3-mini")
        Settings.llm = OpenAI(temperature=0, model="o3-mini")
    elif "klO8n1OFxLWBoPIeDycM" in api_key:
        print("Model: gpt-4o-mini")
        Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")
    else:
        print("Model: gpt-4o-mini (default)")
        Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")
    # Settings.llm = OpenAI(temperature=0, model="o3-mini")

    ## Option 2: Read CVE list from file
    cve_lst = []
    with open("../../target_lst/vulhub_test/xstream.txt", 'r') as f:
        cve_lst = f.read().splitlines()

    ## Option 3: Hard-coded CVE list
    # cve_lst = ['CVE-XXX-XXXX', 'CVE-XXX-XXXX', 'CVE-XXX-XXXX', 'CVE-XXX-XXXX', 'CVE-XXX-XXXX']
    

    ### Step 3: Analyze the CVEs
    output_dir = "resources_test/xstream"
    os.makedirs(output_dir, exist_ok=True)

    ## Scenario 1: Given a list of CVEs, analyze the CVEs

    # analyze_cve_lst(cve_lst, output_dir)
    analyze_cve_lst(cve_lst, output_dir, "xstream")  # assume app's name is xstream


    if track_tokens:
        print(
            "Embedding Tokens: ",
            token_counter.total_embedding_token_count,
            "\n",
            "LLM Prompt Tokens: ",
            token_counter.prompt_llm_token_count,
            "\n",
            "LLM Completion Tokens: ",
            token_counter.completion_llm_token_count,
            "\n",
            "Total LLM Token Count: ",
            token_counter.total_llm_token_count,
            "\n",
            "Average LLM Token Count: ",
            token_counter.total_llm_token_count / len(cve_lst),
            "\n",
        )

        logging.info(
            "Embedding Tokens: "
            + str(token_counter.total_embedding_token_count)
            + "\n"
            + "LLM Prompt Tokens: "
            + str(token_counter.prompt_llm_token_count)
            + "\n"
            + "LLM Completion Tokens: "
            + str(token_counter.completion_llm_token_count)
            + "\n"
            + "Total LLM Token Count: "
            + str(token_counter.total_llm_token_count)
            + "\n"
            + "Average LLM Token Count: "
            + str(token_counter.total_llm_token_count / len(cve_lst))
            + "\n"
        )

        token_counter.reset_counts()
    

    # epss_csv_path = 'epss_scores-2024-08-06.csv'
    # cve_analysis_from_epss_csv(epss_csv_path)

 

if __name__ == "__main__":
    main()

